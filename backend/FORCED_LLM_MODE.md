# üöÄ Forced LLM Mode

**Date:** October 22, 2025
**Status:** ‚úÖ **ACTIVE**

---

## üéØ Overview

Forced LLM Mode **skips complexity assessment** and routes **ALL inputs** to the Gemini LLM, regardless of input simplicity.

---

## üîÑ What Changed

### Before (Hybrid Routing)
```
Input received
  ‚Üì
Assess complexity (0-100 score)
  ‚Üì
Simple (score >= 50) ‚Üí Regex extraction (FREE, fast)
Complex (score < 50) ‚Üí LLM planning ($0.0002, slower)
  ‚Üì
Route to agents
```

**Cost:** ~30% requests use LLM

### After (Forced LLM)
```
Input received
  ‚Üì
[SKIP complexity assessment]
  ‚Üì
ALWAYS ‚Üí LLM planning (Gemini 2.0 Flash)
  ‚Üì
Route to agents
```

**Cost:** 100% requests use LLM

---

## ‚öôÔ∏è Configuration

### Enable/Disable

**File:** `app/core/config.py`

```python
# Routing Strategy
FORCE_LLM_ROUTING: bool = True  # Set to False to restore hybrid routing
```

**Environment Variable:**
```bash
# .env file
FORCE_LLM_ROUTING=true   # Enable forced LLM
FORCE_LLM_ROUTING=false  # Restore hybrid routing
```

---

## üìä How It Works

### Code Implementation

**File:** `app/services/smart_input_router.py`

```python
async def process_input(user_input, image_description, vision_confidence):
    # Check configuration
    if settings.FORCE_LLM_ROUTING:
        # Skip complexity assessment
        logger.info("Forced LLM routing enabled")
        log_party_event("Forced LLM routing enabled - skipping complexity assessment")

        # Always use LLM
        result = await self._process_with_llm(user_input, image_description, vision_confidence)

        result["routing"] = {
            "forced_llm": True,
            "processor": "llm",
            "vision_confidence": vision_confidence
        }

        return result

    # Original hybrid routing (if flag is False)
    complexity = self._assess_complexity(user_input, image_description)
    ...
```

---

## üß™ Testing Results

### Test 1: Simple Input

**Before (Hybrid):** Would use REGEX (free)
**After (Forced LLM):** Uses Gemini LLM

```python
Input: "Birthday party for 30 kids, age 8, superhero theme"

Result:
‚úÖ Processor: LLM
‚úÖ Forced LLM: True
‚úÖ Extracted Event: birthday party
‚úÖ Extracted Theme: superhero
```

### Test 2: Complex Input

**Before (Hybrid):** Would use LLM
**After (Forced LLM):** Still uses LLM (no change)

```python
Input: "My grandmother loves her garden and afternoon tea. She is turning 85."

Result:
‚úÖ Processor: LLM
‚úÖ Forced LLM: True
‚úÖ Extracted Event: birthday
‚úÖ Extracted Theme: garden tea party
```

### Test 3: With Vision

```python
Text: "I want something like this for my daughter"
Image: "Pink unicorn birthday cake with rainbow frosting and edible glitter"
Vision Confidence: 0.92

Result:
‚úÖ Processor: LLM
‚úÖ Forced LLM: True
‚úÖ Vision Confidence: 0.92
‚úÖ Extracted Theme: unicorn
```

---

## üìÑ Party Logging

### Log Entries

Forced LLM mode creates specific log entries:

```json
{
  "timestamp": "2025-10-22T...",
  "level": "INFO",
  "party_id": "fp-test-party",
  "message": "Forced LLM routing enabled - skipping complexity assessment",
  "has_vision": false,
  "input_preview": "Birthday party for 30 kids, age 8, superhero theme"
}

{
  "timestamp": "2025-10-22T...",
  "level": "INFO",
  "party_id": "fp-test-party",
  "message": "Processing with LLM planner",
  "vision_confidence": null
}
```

**No complexity assessment logged** - complexity scoring is skipped entirely.

---

## üí∞ Cost Analysis

### Before (Hybrid Routing)

| Input Type | % of Requests | Processor | Cost/Request |
|------------|---------------|-----------|--------------|
| Simple | 70% | Regex | $0.00 |
| Complex | 30% | Gemini LLM | $0.0002 |
| **Average** | **100%** | **Mixed** | **$0.00006** |

**Monthly cost (10,000 requests):** $0.60

### After (Forced LLM)

| Input Type | % of Requests | Processor | Cost/Request |
|------------|---------------|-----------|--------------|
| All | 100% | Gemini LLM | $0.0002 |
| **Average** | **100%** | **LLM** | **$0.0002** |

**Monthly cost (10,000 requests):** $2.00

**Cost increase:** ~3.3x
**Still cheaper than OpenAI:** 50-150x cheaper than GPT-4 Turbo

---

## üìà Benefits vs. Tradeoffs

### ‚úÖ Benefits

1. **Higher Quality Extraction**
   - Better semantic understanding for all inputs
   - More accurate theme and event type inference
   - Richer inferred requirements

2. **Consistent Behavior**
   - All inputs processed the same way
   - No complexity threshold edge cases
   - Predictable quality

3. **Better Implicit Needs Detection**
   - Simple inputs like "birthday for Emma" get enriched with age-appropriate suggestions
   - More comprehensive agent instructions

4. **Simplified Debugging**
   - No complexity scoring to debug
   - Single code path for all inputs

### ‚ö†Ô∏è Tradeoffs

1. **Higher Cost**
   - 3.3x more expensive than hybrid
   - $2/month vs $0.60/month (for 10k requests)

2. **Slower Response**
   - All requests take 3-6 seconds (LLM latency)
   - No instant regex responses

3. **More API Calls**
   - 100% of requests hit Gemini API
   - Higher quota usage

---

## üéØ When to Use Forced LLM

### ‚úÖ Use Forced LLM When:

- **Quality is priority** - You want best extraction for all inputs
- **Low traffic** - <50k requests/month (cost is negligible)
- **Testing** - Evaluating LLM quality before optimizing
- **User feedback indicates** - Simple inputs aren't being processed well

### ‚õî Don't Use Forced LLM When:

- **High traffic** - >100k requests/month (cost adds up)
- **Speed is critical** - Need sub-second responses
- **Budget constrained** - Every penny counts
- **Regex works well** - Simple inputs are being extracted correctly

---

## üîÑ Switching Back to Hybrid

To restore the original hybrid routing:

**1. Update Config:**
```python
# app/core/config.py
FORCE_LLM_ROUTING: bool = False
```

**2. Restart Server:**
```bash
# Restart to pick up new config
./start_server.sh
```

**3. Verify:**
```bash
curl -X POST http://localhost:9000/api/v1/event-driven/party/test/input \
  -H "Content-Type: application/json" \
  -d '{"content": "Birthday party for 30 kids, superhero theme", "source_type": "text"}'

# Check logs - should see "Input complexity assessed" instead of "Forced LLM routing"
```

---

## üìä Routing Metadata

### Response Format

**With Forced LLM:**
```json
{
  "extracted_data": {...},
  "natural_language": "...",
  "confidence": 46.5,
  "routing": {
    "forced_llm": true,
    "processor": "llm",
    "vision_confidence": null
  }
}
```

**With Hybrid Routing:**
```json
{
  "extracted_data": {...},
  "natural_language": "...",
  "confidence": 55.0,
  "routing": {
    "complexity": {
      "level": "simple",
      "use_llm": false,
      "reasons": ["explicit_theme", "concise_input"]
    },
    "processor": "regex",
    "vision_confidence": null
  }
}
```

---

## üß∞ API Impact

### No Breaking Changes

The API response structure is **identical** whether forced LLM is enabled or not:

- ‚úÖ Same `extracted_data` structure
- ‚úÖ Same `natural_language` field
- ‚úÖ Same `confidence` score
- ‚úÖ Same agent routing

**Only difference:** `routing` metadata includes `forced_llm: true`

---

## üìù Example Usage

### Python

```python
from app.services.smart_input_router import get_smart_router

router = get_smart_router()

# Simple input - will use LLM with forced mode
result = await router.process_input(
    "Birthday party for kids",
    None,
    None
)

print(result["routing"]["processor"])  # "llm"
print(result["routing"]["forced_llm"])  # True
```

### cURL

```bash
# Create party
curl -X POST http://localhost:9000/api/v1/event-driven/party \
  -H "Content-Type: application/json" \
  -d '{"party_id": "test-forced-llm"}'

# Add simple input (will use LLM)
curl -X POST http://localhost:9000/api/v1/event-driven/party/fp-test-forced-llm/input \
  -H "Content-Type: application/json" \
  -d '{"content": "Birthday for 20 kids", "source_type": "text"}'

# Check logs
curl http://localhost:9000/api/v1/event-driven/party/fp-test-forced-llm/logs

# You'll see: "Forced LLM routing enabled - skipping complexity assessment"
```

---

## üéì FAQ

### Q: Will this break my existing code?
**A:** No. The API response format is identical. Only the routing decision changes.

### Q: Can I toggle this per-request?
**A:** Not currently. It's a global setting. You'd need to restart the server to change it.

### Q: Does this affect vision analysis?
**A:** No. Vision analysis still works the same way. Images are still processed by vision API.

### Q: What about agent routing?
**A:** No change. Agents are routed based on the LLM plan, same as before.

### Q: Is the data extraction better?
**A:** Generally yes. LLM provides richer semantic understanding and better inferred requirements.

### Q: What's the performance impact?
**A:** All requests take 3-6 seconds (LLM latency) instead of <100ms for regex requests.

---

## ‚úÖ Verification Checklist

- [x] Configuration flag added to `config.py`
- [x] Smart router updated to check flag
- [x] Party logging updated with forced LLM messages
- [x] Tests passing with forced LLM enabled
- [x] Vision integration still works
- [x] No breaking changes to API
- [x] Documentation complete

---

## üìö Related Documentation

- `ROUTING_AND_LOGGING_GUIDE.md` - Original hybrid routing approach
- `GEMINI_LLM_INTEGRATION.md` - Gemini integration details
- `PARTY_LOGGING_GUIDE.md` - Party-specific logging

---

## ‚úÖ Status

**FORCED LLM MODE: ACTIVE** üöÄ

Current configuration:
- ‚úÖ `FORCE_LLM_ROUTING = True`
- ‚úÖ All inputs routed to Gemini LLM
- ‚úÖ Complexity assessment skipped
- ‚úÖ Vision integration working
- ‚úÖ Party logging tracking forced mode

**Ready for production use!**
